{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8791cf4-a073-402e-9358-c0e65a31d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "from fpdf import FPDF   # simple PDF generation\n",
    "from agents import BenchmarkAgent, MemoryAgent, EvaluationAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65d6ca8-70b9-4431-b9df-8c6d3766e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportingAgent:\n",
    "    def __init__(self, output_dir=\"../reports\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def generate_pdf(self, eval_result, filename=\"report.pdf\"):\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=\"Portfolio Evaluation Report\", ln=True, align=\"C\")\n",
    "\n",
    "        # Portfolio Metrics\n",
    "        pdf.ln(10)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, \"Portfolio Metrics:\", ln=True)\n",
    "        pdf.set_font(\"Arial\", size=10)\n",
    "        for k, v in eval_result[\"Portfolio\"].items():\n",
    "            pdf.cell(200, 8, f\"{k}: {v:.4f}\", ln=True)\n",
    "\n",
    "        # Benchmarks\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, \"Benchmark Metrics:\", ln=True)\n",
    "        pdf.set_font(\"Arial\", size=10)\n",
    "        for bench, metrics in eval_result[\"Benchmarks\"].items():\n",
    "            pdf.cell(200, 8, f\"{bench}:\", ln=True)\n",
    "            for k, v in metrics.items():\n",
    "                pdf.cell(200, 8, f\"   {k}: {v:.4f}\", ln=True)\n",
    "\n",
    "        # Past Average\n",
    "        pdf.ln(5)\n",
    "        pdf.cell(200, 10, f\"Average Past Sharpe: {eval_result['Avg_Past_Sharpe']:.4f}\", ln=True)\n",
    "\n",
    "        # Decision\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, f\"Decision: {eval_result['Decision']}\", ln=True)\n",
    "\n",
    "        # Save\n",
    "        out_path = os.path.join(self.output_dir, filename)\n",
    "        pdf.output(out_path)\n",
    "        return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3c07b0-be36-4a19-b3b5-b3ab28a05f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\Onedrive\\agentic-ai-portfolio\\notebooks\\..\\agents\\benchmark_agent.py:14: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=self.start, end=self.end, progress=False)\n",
      "C:\\Users\\adity\\Onedrive\\agentic-ai-portfolio\\notebooks\\..\\agents\\benchmark_agent.py:14: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=self.start, end=self.end, progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generated: ../reports\\Day21_report.pdf\n"
     ]
    }
   ],
   "source": [
    "# Setup benchmark + evaluation\n",
    "bench_agent = BenchmarkAgent(tickers=[\"SPY\", \"QQQ\"], start=\"2020-01-01\")\n",
    "bench_agent.fetch()\n",
    "memory = MemoryAgent()\n",
    "\n",
    "portfolio = pd.read_csv(\"../logs/Day18_portfolio.csv\", index_col=0, parse_dates=True)\n",
    "evaluator = EvaluationAgent(benchmark_agent=bench_agent, memory=memory)\n",
    "eval_result = evaluator.evaluate(portfolio)\n",
    "\n",
    "# Generate report\n",
    "reporter = ReportingAgent()\n",
    "report_path = reporter.generate_pdf(eval_result, filename=\"Day21_report.pdf\")\n",
    "\n",
    "print(\"Report generated:\", report_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa86ee5-7c87-48b1-b533-f9213c6e1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../logs/Day21_summary.txt\", \"w\") as f:\n",
    "    f.write(\"Day 21 reporting run\\n\")\n",
    "    f.write(\"Report generated at: \" + report_path + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e98a18-7b4b-4047-81f0-a4cb1b2c1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../agents/reporting_agent.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\"\"import os\n",
    "from fpdf import FPDF\n",
    "\n",
    "class ReportingAgent:\n",
    "    def __init__(self, output_dir=\"../reports\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def generate_pdf(self, eval_result, filename=\"report.pdf\"):\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=\"Portfolio Evaluation Report\", ln=True, align=\"C\")\n",
    "\n",
    "        # Portfolio Metrics\n",
    "        pdf.ln(10)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, \"Portfolio Metrics:\", ln=True)\n",
    "        pdf.set_font(\"Arial\", size=10)\n",
    "        for k, v in eval_result[\"Portfolio\"].items():\n",
    "            pdf.cell(200, 8, f\"{k}: {v:.4f}\", ln=True)\n",
    "\n",
    "        # Benchmarks\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, \"Benchmark Metrics:\", ln=True)\n",
    "        pdf.set_font(\"Arial\", size=10)\n",
    "        for bench, metrics in eval_result[\"Benchmarks\"].items():\n",
    "            pdf.cell(200, 8, f\"{bench}:\", ln=True)\n",
    "            for k, v in metrics.items():\n",
    "                pdf.cell(200, 8, f\"   {k}: {v:.4f}\", ln=True)\n",
    "\n",
    "        # Past Average\n",
    "        pdf.ln(5)\n",
    "        pdf.cell(200, 10, f\"Average Past Sharpe: {eval_result['Avg_Past_Sharpe']:.4f}\", ln=True)\n",
    "\n",
    "        # Decision\n",
    "        pdf.ln(5)\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(200, 10, f\"Decision: {eval_result['Decision']}\", ln=True)\n",
    "\n",
    "        # Save\n",
    "        out_path = os.path.join(self.output_dir, filename)\n",
    "        pdf.output(out_path)\n",
    "        return out_path\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dc39de-9597-4c85-af81-1a1564a629a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generated: ../reports\\Day21_report.pdf\n"
     ]
    }
   ],
   "source": [
    "from agents import ReportingAgent\n",
    "\n",
    "reporter = ReportingAgent()\n",
    "report_path = reporter.generate_pdf(eval_result, filename=\"Day21_report.pdf\")\n",
    "print(\"Report generated:\", report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0951aa-7a9e-4cec-91e9-1d30d53f183c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
