{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68936cbd-dd35-4b76-86fe-f61fb894202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from agents import StrategyAgent, RiskAgent, AllocatorAgent, ReflectionAgent, MemoryAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b52cfd-2a26-4e1b-b031-bb87c508b557",
   "metadata": {},
   "source": [
    "EvaluationAgent takes portfolio metrics.\n",
    "\n",
    "Compares them to:\n",
    "\n",
    "Past portfolios (via MemoryAgent).\n",
    "\n",
    "Benchmark returns (e.g., SPY).\n",
    "\n",
    "Uses rules:\n",
    "\n",
    "Adopt if Sharpe ratio is higher than both past average and benchmark.\n",
    "\n",
    "Reject if worse than both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bc2e5b-3146-4b55-96f4-d424c0e4c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationAgent:\n",
    "    def __init__(self, benchmark_file=\"../data/SPY_sample.csv\"):\n",
    "        self.benchmark_file = benchmark_file\n",
    "        self.benchmark = None\n",
    "        if os.path.exists(benchmark_file):\n",
    "            self.benchmark = pd.read_csv(benchmark_file, index_col=0, parse_dates=True)\n",
    "    \n",
    "    def compute_metrics(self, returns):\n",
    "        strat_curve = (1 + returns.fillna(0)).cumprod()\n",
    "        years = (returns.index[-1] - returns.index[0]).days / 365.25\n",
    "        cagr = strat_curve.iloc[-1]**(1/years) - 1 if years > 0 else 0\n",
    "        vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = (returns.mean() * 252) / (returns.std() * np.sqrt(252)) if returns.std() > 0 else 0\n",
    "        roll_max = strat_curve.cummax()\n",
    "        dd = (strat_curve / roll_max - 1).min()\n",
    "        return {\"CAGR\": cagr, \"Volatility\": vol, \"Sharpe\": sharpe, \"MaxDD\": dd}\n",
    "    \n",
    "    def evaluate(self, portfolio_df, memory: MemoryAgent):\n",
    "        # Current portfolio metrics\n",
    "        portfolio_metrics = self.compute_metrics(portfolio_df[\"Portfolio_Return\"])\n",
    "        \n",
    "        # Benchmark metrics\n",
    "        if self.benchmark is not None:\n",
    "            bench_metrics = self.compute_metrics(self.benchmark[\"Return\"])\n",
    "        else:\n",
    "            bench_metrics = {\"Sharpe\": 0, \"CAGR\": 0, \"Volatility\": 0, \"MaxDD\": 0}\n",
    "        \n",
    "        # Historical averages from MemoryAgent\n",
    "        history_df = pd.DataFrame(memory.history)\n",
    "        if not history_df.empty and \"Metrics\" in history_df.columns:\n",
    "            past_sharpes = [m[\"Sharpe\"] for m in history_df[\"Metrics\"] if m is not None and \"Sharpe\" in m]\n",
    "            avg_past_sharpe = np.mean(past_sharpes) if past_sharpes else 0\n",
    "        else:\n",
    "            avg_past_sharpe = 0\n",
    "        \n",
    "        # Decision logic\n",
    "        decision = \"ADOPT\" if (portfolio_metrics[\"Sharpe\"] > bench_metrics[\"Sharpe\"] and\n",
    "                               portfolio_metrics[\"Sharpe\"] > avg_past_sharpe) else \"REJECT\"\n",
    "        \n",
    "        return {\n",
    "            \"Portfolio\": portfolio_metrics,\n",
    "            \"Benchmark\": bench_metrics,\n",
    "            \"Avg_Past_Sharpe\": avg_past_sharpe,\n",
    "            \"Decision\": decision\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e50845-1d1b-4ada-ad86-156800b1467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: {'Portfolio': {'CAGR': np.float64(0.1398491088229432), 'Volatility': np.float64(0.04382244081992615), 'Sharpe': np.float64(2.994467401985038), 'MaxDD': np.float64(-0.007035576834643953)}, 'Benchmark': {'Sharpe': 0, 'CAGR': 0, 'Volatility': 0, 'MaxDD': 0}, 'Avg_Past_Sharpe': 0, 'Decision': 'ADOPT'}\n"
     ]
    }
   ],
   "source": [
    "# Load yesterdayâ€™s saved portfolio (from Day 18 logs)\n",
    "portfolio = pd.read_csv(\"../logs/Day18_portfolio.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Load memory (Day 18 log)\n",
    "memory = MemoryAgent()\n",
    "history_path = \"../logs/Day18_memory.csv\"\n",
    "if os.path.exists(history_path):\n",
    "    df = pd.read_csv(history_path)\n",
    "    for _, row in df.iterrows():\n",
    "        memory.history.append({\"Strategy\": row[\"Strategy\"], \"Metrics\": None, \"Decision\": row[\"Decision\"], \"Notes\": row[\"Notes\"]})\n",
    "\n",
    "# Evaluate\n",
    "evaluator = EvaluationAgent()\n",
    "result = evaluator.evaluate(portfolio, memory)\n",
    "\n",
    "print(\"Evaluation Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1a7661-8317-41a2-8444-abbdc723c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../logs\", exist_ok=True)\n",
    "\n",
    "pd.DataFrame([result]).to_csv(\"../logs/Day19_evaluation.csv\", index=False)\n",
    "\n",
    "with open(\"../logs/Day19_summary.txt\", \"w\") as f:\n",
    "    f.write(\"Day 19 evaluation run\\n\")\n",
    "    f.write(str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62406be5-4fde-4cd1-87f1-9a5758116aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
